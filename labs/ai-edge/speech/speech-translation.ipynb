{
 "cells": [
  {
   "source": [
    "# Speech translation using a Raspberry Pi\n",
    "\n",
    "This lab shows how to use the [Azure Cognitive Services speech service](https://azure.microsoft.com/services/cognitive-services/speech-services/?WT.mc_id=academic-7372-jabenn) and [Translation service](https://azure.microsoft.com/services/cognitive-services/translator/?WT.mc_id=academic-7372-jabenn) on a Raspberry Pi. You will need a Cognitive Services speech resource, as well as a Cognitive Services Translation resource to use this lab, and you can find all the instructions to get set up in the [README file](https://github.com/microsoft/iot-curriculum/tree/main/labs/ai-edge/speech).\n",
    "\n",
    "This lab records 10 seconds of speech, then sends it to the Speech service to convert to text. The text is then translated to another languges using the Translation service, then converted back to speech using the Speech service.\n",
    "\n",
    "There is currently no SDK support for this speech service on ARM32 Linux, so this lab uses the REST APIs.\n",
    "\n",
    "To use this Notebook, read each documentation cell, then select Run to run each code cell. The output of the code cells will be shown below. You can read more on running Jupyter Notebooks in the [Jupyter Notebooks documentation](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First the options for the Speech cognitive service need to be configured.\n",
    "* Set the `SPEECH_KEY` variable to be the key of your speech resource.\n",
    "* Set the `SPEECH_ENDPOINT` variable to be the endpoint of your speech resource.\n",
    "* Set the `TRANSLATOR_KEY` variable to be the key of your translator resource.\n",
    "* Set the `INPUT_LANGUAGE` variable to the language you will be speaking in. You can find details on the supported langauges in the [Language and voice support for the Speech service documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/language-support?WT.mc_id=academic-7372-jabenn).\n",
    "* Set the `OUTPUT_LANGUAGE` to the language you want your speech translated to."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_KEY = \"YOUR_SPEECH_KEY\"\n",
    "SPEECH_ENDPOINT = \"YOUR_SPEECH_ENDPOINT\"\n",
    "TRANSLATOR_KEY = \"YOUR_TRANSLATOR_KEY\"\n",
    "INPUT_LANGUAGE = \"en-US\"\n",
    "OUTPUT_LANGUAGE = \"zh-CN\""
   ]
  },
  {
   "source": [
    "Import some Python packages to hande the microphone, audio files and REST requests to make them available to the Python code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from scipy.io.wavfile import write\n",
    "from io import BytesIO"
   ]
  },
  {
   "source": [
    "Before audio can be captured, some configuration needs to be set up. The sample rate needs to be set to 16khz, and the sample length needs to be set to 10 seconds.\n",
    "\n",
    "> If you want to record for longer, change the value of `sample_len` to the time in seconds that you want to record for."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Speech to Text Cognitive Service API currently only supports a 16000hz samplerate\n",
    "sample_rate = 16000\n",
    "\n",
    "# Length of the audio sample in seconds\n",
    "sample_len = 10"
   ]
  },
  {
   "source": [
    "Now capture the audio. Once you start running this cell, speak into the microphone for 10 seconds."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the speech sample\n",
    "speech_sample = sd.rec(int(sample_len * sample_rate), samplerate=sample_rate, channels=1)\n",
    "\n",
    "print(\"Start speaking now!\")\n",
    "\n",
    "# Wait for the recording to stop after the specified number of seconds\n",
    "sd.wait()\n",
    "\n",
    "# Let the user know the recording is done\n",
    "print(\"Recorded!\")"
   ]
  },
  {
   "source": [
    "The speech sample now needs to be saved to disk."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of audio file to save sample\n",
    "filename = \"speech_to_text_rec.wav\"\n",
    "\n",
    "# Save speech sample as a .wav file\n",
    "write(filename, sample_rate, speech_sample)"
   ]
  },
  {
   "source": [
    "To verify everything was recorded correctly, playback the audio by using the `aplay` command line utility"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"aplay \" + filename)"
   ]
  },
  {
   "source": [
    "The endpoint that comes from the Speech Cognitive Service is designed to issue access tokens so you can then make the relevant API call. \n",
    "\n",
    "The REST API is documented in the [Speech-to-text REST API documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/rest-speech-to-text?WT.mc_id=academic-7372-jabenn#authentication).\n",
    "\n",
    "The header passes the following:\n",
    "\n",
    "* The API Key of the speech resource\n",
    "\n",
    "The return value is an access token that lasts for 10 minutes and is used when calling the rest of the API."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the request headers with the API key\n",
    "headers = {\n",
    "    \"Ocp-Apim-Subscription-Key\": SPEECH_KEY\n",
    "}\n",
    "\n",
    "# Make a POST request to the endpoint to get the token\n",
    "response = requests.post(SPEECH_ENDPOINT, headers=headers)\n",
    "speech_access_token = str(response.text)"
   ]
  },
  {
   "source": [
    "The same needs to happen for the Translator service to get an access token. Translators can be global, so don't have a service or region specific endpoint.\n",
    "\n",
    "The REST API is documented in the [Translator V3.0 REST API documentation](https://docs.microsoft.com/azure/cognitive-services/translator/reference/v3-0-reference?WT.mc_id=academic-7372-jabenn#authenticating-with-an-access-token).\n",
    "\n",
    "The header passes the following:\n",
    "\n",
    "* The API Key of the translator resource\n",
    "\n",
    "The return value is an access token that lasts for 10 minutes and is used when calling the rest of the API."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the request headers with the API key\n",
    "headers = {\n",
    "    \"Ocp-Apim-Subscription-Key\": TRANSLATOR_KEY\n",
    "}\n",
    "\n",
    "# Make a POST request to the endpoint to get the token\n",
    "response = requests.post(\"https://api.cognitive.microsoft.com/sts/v1.0/issueToken\", headers=headers)\n",
    "translator_access_token = str(response.text)"
   ]
  },
  {
   "source": [
    "All future API calls to the speech service will need to be at the same endpoint as the token issuer, so extract the location now"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the location from the endpoint by removing the http protocol and getting the section before the first .\n",
    "speech_location = SPEECH_ENDPOINT.split(\"//\")[-1].split(\".\")[0]"
   ]
  },
  {
   "source": [
    "Next step is to make the REST API call, uploading the file with the speech data to a URL. The URL is built by extracting the location from the API endpoint and using that to build a new URL pointing to the speech service itself.\n",
    "\n",
    "The REST API is documented in the [Speech-to-text REST API documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/rest-speech-to-text?WT.mc_id=academic-7372-jabenn#sample-request).\n",
    "\n",
    "The header passes the following:\n",
    "\n",
    "* The bearer token that was retrieved earlier\n",
    "* The content type as a WAV file with a sample rate of 16KHz\n",
    "\n",
    "The body of the request is the audio file that was just written.\n",
    "\n",
    "The return value is a JSON document with details on the detected speech, including the text from the speech."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the URL from the location\n",
    "url = \"https://\" + speech_location + \".stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1\"\n",
    "\n",
    "# Set the headers to include the Cognitive Services resource key\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + speech_access_token,\n",
    "    \"Content-Type\": \"audio/wav; codecs=audio/pcm; samplerate=16000\",\n",
    "    \"Accept\": \"application/json;text/xml\"\n",
    "}\n",
    "\n",
    "# Configure the language parameter for the call\n",
    "params = {\n",
    "    \"language\": INPUT_LANGUAGE\n",
    "}\n",
    "\n",
    "# Make the request passing the file as the body\n",
    "response = requests.post(url, headers=headers, params=params, data=open(filename, \"rb\"))"
   ]
  },
  {
   "source": [
    "The `response` contains the result of the speech to text call as JSON. If the call was successful, it will return an object with a `RecognitionStatus` of `Success`, and a `DisplayText` with the speech converted to text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the response to JSON\n",
    "response_json = json.loads(response.text)\n",
    "\n",
    "# Get the text from the speech\n",
    "output_text = response_json['DisplayText']\n",
    "\n",
    "print(\"Output text:\", output_text)"
   ]
  },
  {
   "source": [
    "The next step is to translate the text into the output language using another REST API call.\n",
    "\n",
    "The REST API is documented in the [Translator Translate REST API documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/translator/reference/v3-0-translate?WT.mc_id=academic-7372-jabenn).\n",
    "\n",
    "The header passes the following:\n",
    "\n",
    "* The bearer token that was retrieved earlier\n",
    "* The content type as JSON\n",
    "* The content length of the length of the body\n",
    "\n",
    "The body of the request is a JSON document listing the text to translate.\n",
    "\n",
    "The return value is a JSON document with details on the translated speech."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Translator URL\n",
    "url = \"https://api.cognitive.microsofttranslator.com/translate?api-version=3.0\"\n",
    "\n",
    "# Build JSON containing the translation request\n",
    "translation_request_json = [\n",
    "    { \"Text\" : output_text }\n",
    "]\n",
    "\n",
    "translation_request_body = json.dumps(translation_request_json)\n",
    "\n",
    "# Set the headers to include the Cognitive Services resource key\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + translator_access_token,\n",
    "    \"Content-Type\": \"application/json; charset=UTF-8\",\n",
    "    \"Content-Length\": str(len(translation_request_body))\n",
    "}\n",
    "\n",
    "# Configure the language parameter for the call\n",
    "params = {\n",
    "    \"from\": INPUT_LANGUAGE,\n",
    "    \"to\": OUTPUT_LANGUAGE\n",
    "}\n",
    "\n",
    "# Make the request passing the translation request\n",
    "response = requests.post(url, headers=headers, params=params, data=translation_request_body)"
   ]
  },
  {
   "source": [
    "Once the translation response has been received, the translated text can be extracted from it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the response body to JSON\n",
    "response_json = json.loads(response.text)\n",
    "\n",
    "# The response contains an array of translations. We're only translating to one language, so will only have one item in this array\n",
    "translation = response_json[0][\"translations\"][0]\n",
    "\n",
    "# Get the text of the translation\n",
    "translated_text = translation[\"text\"]\n",
    "\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "source": [
    "The translated text can then be converted to speech using the Text to speech REST API. The first step in doing this is to get the list of voices that are supported by the text to speech service. This list can then be filtered based on the specified output language, selecting the first one found.\n",
    "\n",
    "The REST API is documented in the [Text-to-speech REST API documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/rest-text-to-speech?WT.mc_id=academic-7372-jabenn#get-a-list-of-voices).\n",
    "\n",
    "The header passes the following:\n",
    "\n",
    "* The access token that was retrieved earlier\n",
    "\n",
    "The return value is a JSON document listing all the supported voices.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the URL from the location\n",
    "url = \"https://\" + speech_location + \".tts.speech.microsoft.com/cognitiveservices/voices/list\"\n",
    "\n",
    "# Set the headers to include the Cognitive Services resource key\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + speech_access_token\n",
    "}\n",
    "\n",
    "# Make the request passing the file as the body\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "voices_json = json.loads(response.text)\n",
    "\n",
    "# Pick the first voice that matches the language\n",
    "voice = next(x for x in voices_json if x[\"Locale\"].lower() == OUTPUT_LANGUAGE.lower())"
   ]
  },
  {
   "source": [
    "Next step is to make the REST API call, uploading the text to a URL. The URL is built by extracting the location from the API endpoint and using that to build a new URL pointing to the speech service itself.\n",
    "\n",
    "The REST API is documented in the [Text-to-speech REST API documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/rest-text-to-speech?WT.mc_id=academic-7372-jabenn#convert-text-to-speech).\n",
    "\n",
    "The header passes the following:\n",
    "\n",
    "* The bearer token that was retrieved earlier\n",
    "* The content type as SSML\n",
    "* A requested output format of 16KHz Mono WAV file\n",
    "\n",
    "The body of the request is an SSML document detailing the text to convert. SSML is Speech Synthesis Markup Language, and is an XML-based markup language that lets developers specify how input text is converted into synthesized speech using the text-to-speech service. You can read more on SSML in the [Improve synthesis with Speech Synthesis Markup Language (SSML) documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/speech-synthesis-markup?WT.mc_id=academic-7372-jabenn). The SSML needs to include details on the voice to use to generate the speech, so extract these from the voice found earlier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the URL from the location\n",
    "url = \"https://\" + speech_location + \".tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "\n",
    "# Set the headers to include the Cognitive Services resource key\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + speech_access_token,\n",
    "    \"Content-Type\": \"application/ssml+xml\",\n",
    "    \"X-Microsoft-OutputFormat\": \"riff-16khz-16bit-mono-pcm\"\n",
    "}\n",
    "\n",
    "# Build the SSML\n",
    "ssml =  \"<speak version='1.0' xml:lang='\" + OUTPUT_LANGUAGE + \"'>\"\n",
    "ssml += \"  <voice  xml:lang='\" + OUTPUT_LANGUAGE + \"' xml:gender='\" + voice[\"Gender\"] + \"' name='\" + voice[\"ShortName\"] + \"'>\"\n",
    "ssml += translated_text\n",
    "ssml += \"  </voice>\"\n",
    "ssml += \"</speak>\"\n",
    "\n",
    "# Make the request passing the file as the body encoded as unicode to handle all languages\n",
    "response = requests.post(url, headers=headers, data=ssml.encode(\"utf-8\"))"
   ]
  },
  {
   "source": [
    "The `response` contains the result of the text to speech call as binary audio data. This binary data can be saed to a WAV file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the binary data from the response\n",
    "response_audio = BytesIO(response.content)\n",
    "\n",
    "filename = \"translation_output.wav\"\n",
    "\n",
    "with open(filename, \"wb\") as f:\n",
    "    f.write(response_audio.getbuffer())"
   ]
  },
  {
   "source": [
    "Play the speech audio file by using the `aplay` command line utility."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"aplay \" + filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}