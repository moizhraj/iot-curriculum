{
 "cells": [
  {
   "source": [
    "# Text to Speech using a Raspberry Pi\n",
    "\n",
    "This lab shows how to use the [Azure Cognitive Services speech service](https://azure.microsoft.com/services/cognitive-services/speech-services/?WT.mc_id=academic-7372-jabenn) on a Raspberry Pi. You will need a Cognitive Services speech resource to use this lab, and you can find all the instructions to get set up in the [README file](https://github.com/microsoft/iot-curriculum/tree/main/labs/ai-edge/speech).\n",
    "\n",
    "This lab takes some text, then sends it to the Speech service to convert to speech as an audio file.\n",
    "\n",
    "There is currently no SDK support for this speech service on ARM32 Linux, so this lab uses the REST APIs.\n",
    "\n",
    "To use this Notebook, read each documentation cell, then select Run to run each code cell. The output of the code cells will be shown below. You can read more on running Jupyter Notebooks in the [Jupyter Notebooks documentation](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First the options for the Speech cognitive service need to be configured.\n",
    "* Set the `KEY` variable to be the key of your speech resource.\n",
    "* Set the `ENDPOINT` variable to be the endpoint of your speech resource.\n",
    "* Set the `LANGUAGE` variable to the language for the text to convert. You can find details on the supported langauges in the [Language and voice support for the Speech service documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/language-support?WT.mc_id=academic-7372-jabenn).\n",
    "* Set the `TEXT` variable to the text to convert to speech"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = \"YOUR_SPEECH_KEY\"\n",
    "ENDPOINT = \"YOUR_SPEECH_ENDPOINT\"\n",
    "LANGUAGE = \"en-US\"\n",
    "TEXT = \"At Microsoft, our mission is to empower every person and every organization on the planet to achieve more.\""
   ]
  },
  {
   "source": [
    "Import some Python packages to make them available to the Python code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from io import BytesIO"
   ]
  },
  {
   "source": [
    "The endpoint that comes from the Cognitive Service is designed to issue access tokens so you can then make the relevant API call. \n",
    "\n",
    "The REST API is documented in the [Text-to-speech REST API documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/rest-text-to-speech?WT.mc_id=academic-7372-jabenn#authentication).\n",
    "\n",
    "The header passes the following:\n",
    "\n",
    "* The API Key of the speech resource\n",
    "\n",
    "The return value is an access token that lasts for 10 minutes and is used when calling the rest of the API."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the request headers with the API key\n",
    "headers = {\n",
    "    \"Ocp-Apim-Subscription-Key\": KEY\n",
    "}\n",
    "\n",
    "# Make a POST request to the endpoint to get the token\n",
    "response = requests.post(ENDPOINT, headers=headers)\n",
    "access_token = str(response.text)"
   ]
  },
  {
   "source": [
    "All future API calls will need to be at the same endpoint as the token issuer, so extract the location now"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the location from the endpoint by removing the http protocol and getting the section before the first .\n",
    "location = ENDPOINT.split(\"//\")[-1].split(\".\")[0]"
   ]
  },
  {
   "source": [
    "Next get the list of voices that are supported by the text to speech service. This list can then be filtered based on the specified language, selecting the first one found.\n",
    "\n",
    "The REST API is documented in the [Text-to-speech REST API documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/rest-text-to-speech?WT.mc_id=academic-7372-jabenn#get-a-list-of-voices).\n",
    "\n",
    "The header passes the following:\n",
    "\n",
    "* The access token that was retrieved in the last step\n",
    "\n",
    "The return value is a JSON document listing all the supported voices.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the URL from the location\n",
    "url = \"https://\" + location + \".tts.speech.microsoft.com/cognitiveservices/voices/list\"\n",
    "\n",
    "# Set the headers to include the Cognitive Services resource key\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + access_token\n",
    "}\n",
    "\n",
    "# Make the request passing the file as the body\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "voices_json = json.loads(response.text)\n",
    "\n",
    "# Pick the first voice that matches the language\n",
    "voice = next(x for x in voices_json if x[\"Locale\"].lower() == LANGUAGE.lower())"
   ]
  },
  {
   "source": [
    "Next step is to make the REST API call, uploading the text to a URL. The URL is built by extracting the location from the API endpoint and using that to build a new URL pointing to the speech service itself.\n",
    "\n",
    "The REST API is documented in the [Text-to-speech REST API documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/rest-text-to-speech?WT.mc_id=academic-7372-jabenn#convert-text-to-speech).\n",
    "\n",
    "The header passes the following:\n",
    "\n",
    "* The bearer token that was retrieved earlier\n",
    "* The content type as SSML\n",
    "* A requested output format of 16KHz Mono WAV file\n",
    "\n",
    "The body of the request is an SSML document detailing the text to convert. SSML is Speech Synthesis Markup Language, and is an XML-based markup language that lets developers specify how input text is converted into synthesized speech using the text-to-speech service. You can read more on SSML in the [Improve synthesis with Speech Synthesis Markup Language (SSML) documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/speech-synthesis-markup?WT.mc_id=academic-7372-jabenn). The SSML needs to include details on the voice to use to generate the speech, so extract these from the voice found earlier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the URL from the location\n",
    "url = \"https://\" + location + \".tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "\n",
    "# Set the headers to include the Cognitive Services resource key\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + access_token,\n",
    "    \"Content-Type\": \"application/ssml+xml\",\n",
    "    \"X-Microsoft-OutputFormat\": \"riff-16khz-16bit-mono-pcm\"\n",
    "}\n",
    "\n",
    "# Build the SSML\n",
    "ssml =  \"<speak version='1.0' xml:lang='\" + LANGUAGE + \"'>\"\n",
    "ssml += \"  <voice  xml:lang='\" + LANGUAGE + \"' xml:gender='\" + voice[\"Gender\"] + \"' name='\" + voice[\"ShortName\"] + \"'>\"\n",
    "ssml += TEXT\n",
    "ssml += \"  </voice>\"\n",
    "ssml += \"</speak>\"\n",
    "\n",
    "# Make the request passing the file as the body encoded as unicode to handle all languages\n",
    "response = requests.post(url, headers=headers, data=ssml.encode(\"utf-8\"))"
   ]
  },
  {
   "source": [
    "The `response` contains the result of the text to speech call as binary audio data. This binary data can be saed to a WAV file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the binary data from the response\n",
    "response_audio = BytesIO(response.content)\n",
    "\n",
    "filename = \"text_to_speech_output.wav\"\n",
    "\n",
    "with open(filename, \"wb\") as f:\n",
    "    f.write(response_audio.getbuffer())"
   ]
  },
  {
   "source": [
    "Play the speech audio file by using the `aplay` command line utility."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"aplay \" + filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}